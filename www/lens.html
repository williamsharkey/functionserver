<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Lens - Token-Efficient Code Editing for AI</title>
<meta name="description" content="Lens lets AI edit code surgically‚Äîgrep, view context, edit specific lines, run, and verify. 60x fewer tokens than reading and rewriting files.">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,400;1,6..72,400&display=swap" rel="stylesheet">
<style>
* { margin: 0; padding: 0; box-sizing: border-box; }

:root {
  --bg: #09090b;
  --surface: #18181b;
  --text: #fafafa;
  --text-secondary: #a1a1aa;
  --text-tertiary: #52525b;
  --border: #27272a;
  --accent: #a78bfa;
  --green: #86efac;
  --red: #fca5a5;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', 'Segoe UI', system-ui, sans-serif;
  font-size: 16px;
  line-height: 1.7;
  color: var(--text);
  background: var(--bg);
  -webkit-font-smoothing: antialiased;
}

.grid-bg {
  position: fixed;
  inset: 0;
  z-index: -1;
  background-image:
    linear-gradient(rgba(255,255,255,0.02) 1px, transparent 1px),
    linear-gradient(90deg, rgba(255,255,255,0.02) 1px, transparent 1px);
  background-size: 60px 60px;
  mask-image: radial-gradient(ellipse at 50% 0%, black 0%, transparent 70%);
}

nav {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  padding: 16px 32px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  z-index: 100;
  background: rgba(9, 9, 11, 0.8);
  backdrop-filter: blur(12px);
  border-bottom: 1px solid var(--border);
}

.logo {
  font-size: 14px;
  font-weight: 500;
  color: var(--text);
  text-decoration: none;
}

nav a {
  color: var(--text-secondary);
  text-decoration: none;
  font-size: 13px;
}

nav a:hover { color: var(--text); }

.nav-cta {
  background: var(--text);
  color: var(--bg) !important;
  padding: 8px 16px;
  border-radius: 6px;
  font-weight: 500;
}

article {
  max-width: 720px;
  margin: 0 auto;
  padding: 120px 24px 80px;
}

h1 {
  font-family: 'Newsreader', 'New York', Georgia, serif;
  font-size: clamp(32px, 5vw, 48px);
  font-weight: 400;
  line-height: 1.2;
  margin-bottom: 24px;
  letter-spacing: -0.02em;
}

.lead {
  font-size: 18px;
  color: var(--text-secondary);
  margin-bottom: 48px;
  line-height: 1.8;
}

h2 {
  font-family: 'Newsreader', 'New York', Georgia, serif;
  font-size: 28px;
  font-weight: 400;
  margin-top: 64px;
  margin-bottom: 20px;
  letter-spacing: -0.01em;
}

h3 {
  font-size: 18px;
  font-weight: 500;
  margin-top: 40px;
  margin-bottom: 16px;
  color: var(--accent);
}

p {
  margin-bottom: 20px;
  color: var(--text-secondary);
  line-height: 1.8;
}

strong { color: var(--text); font-weight: 500; }
em { color: var(--text); font-style: italic; }

ol, ul {
  margin-bottom: 20px;
  padding-left: 24px;
  color: var(--text-secondary);
}

li { margin-bottom: 8px; }

code {
  font-family: 'SF Mono', ui-monospace, monospace;
  font-size: 14px;
  background: var(--surface);
  padding: 2px 6px;
  border-radius: 4px;
  color: var(--accent);
}

pre {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 20px 24px;
  margin: 24px 0;
  overflow-x: auto;
  white-space: pre-line;
}

pre code {
  background: none;
  padding: 0;
  font-size: 13px;
  line-height: 1.7;
  color: var(--text);
}

.cm { color: var(--text-tertiary); }
.kw { color: var(--accent); }
.str { color: var(--green); }
.num { color: #fcd34d; }
.result { color: var(--text-tertiary); font-style: italic; }

table {
  width: 100%;
  border-collapse: collapse;
  margin: 24px 0;
  font-size: 14px;
}

th, td {
  padding: 12px 16px;
  text-align: left;
  border-bottom: 1px solid var(--border);
}

th {
  background: var(--surface);
  color: var(--text);
  font-weight: 500;
}

td { color: var(--text-secondary); }
td code { font-size: 12px; }

blockquote {
  background: var(--surface);
  border-left: 3px solid var(--accent);
  padding: 24px 28px;
  margin: 48px 0;
  border-radius: 0 8px 8px 0;
}

blockquote p {
  margin-bottom: 0;
  font-size: 15px;
}

.comparison {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 16px;
  margin: 32px 0;
}

.comparison > div {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 20px;
}

.comparison h4 {
  font-size: 13px;
  text-transform: uppercase;
  letter-spacing: 0.05em;
  margin-bottom: 12px;
  color: var(--text-tertiary);
}

.comparison.bad h4 { color: var(--red); }
.comparison.good h4 { color: var(--green); }

.comparison pre {
  margin: 0;
  padding: 12px;
  font-size: 12px;
}

hr {
  border: none;
  border-top: 1px solid var(--border);
  margin: 48px 0;
}

.cta-section {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 32px;
  margin: 48px 0;
  text-align: center;
}

.cta-section p { margin-bottom: 8px; }
.cta-section a { color: var(--accent); text-decoration: none; }
.cta-section a:hover { text-decoration: underline; }

footer {
  padding: 40px 24px;
  text-align: center;
  border-top: 1px solid var(--border);
}

footer a { color: var(--accent); text-decoration: none; }

.vision-compare {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 24px;
  margin: 32px 0;
}

.vision-compare > div {
  background: var(--surface);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 20px;
}

.vision-compare h4 {
  font-size: 12px;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--text-tertiary);
  margin-bottom: 16px;
  text-align: center;
}

.vision-compare .caption {
  font-size: 12px;
  color: var(--text-tertiary);
  text-align: center;
  margin: 12px 0 0 0;
}

.fake-desktop {
  background: #1a1a2e;
  border-radius: 8px;
  padding: 12px;
  min-height: 160px;
}

.fake-window {
  background: #0d0d0d;
  border-radius: 6px;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.4);
}

.fake-titlebar {
  background: linear-gradient(#3a3a3a, #2a2a2a);
  padding: 8px 12px;
  display: flex;
  align-items: center;
  gap: 8px;
}

.fake-titlebar .dots {
  display: flex;
  gap: 4px;
}

.fake-titlebar .dots span {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background: #ff5f57;
}

.fake-titlebar .dots span:nth-child(2) { background: #ffbd2e; }
.fake-titlebar .dots span:nth-child(3) { background: #28c840; }

.fake-titlebar .title {
  font-size: 11px;
  color: #999;
  margin-left: auto;
  margin-right: auto;
}

.fake-toolbar {
  background: #1a1a1a;
  padding: 6px 10px;
  display: flex;
  gap: 8px;
  border-bottom: 1px solid #333;
}

.fake-toolbar .btn {
  font-size: 10px;
  padding: 3px 8px;
  background: #2a2a2a;
  border-radius: 4px;
  color: #888;
}

.fake-toolbar .btn.active {
  background: var(--accent);
  color: #000;
}

.fake-editor {
  padding: 10px;
  font-family: 'SF Mono', monospace;
  font-size: 11px;
}

.fake-editor .line {
  color: #ccc;
  line-height: 1.6;
}

.fake-editor .ln {
  color: #555;
  margin-right: 12px;
  user-select: none;
}

.claude-view pre {
  margin: 0;
  padding: 12px;
  font-size: 11px;
  line-height: 1.5;
}

@media (max-width: 600px) {
  h1 { font-size: 28px; }
  article { padding: 100px 16px 60px; }
  nav { padding: 16px 20px; }
  .comparison { grid-template-columns: 1fr; }
  .vision-compare { grid-template-columns: 1fr; }
}
</style>
</head>
<body>

<div class="grid-bg"></div>

<nav>
  <a href="/" class="logo">FunctionServer</a>
  <a href="/app" class="nav-cta">Enter</a>
</nav>

<article>
  <h1>Lens: Surgical Code Editing</h1>

  <p class="lead">AI editing code traditionally means reading entire files, generating replacements, and hoping the edit worked. Lens inverts this: grep to find, view context, edit one line, run to verify. <strong>60x fewer tokens.</strong></p>

  <h2>The Core Workflow</h2>

  <p>Three commands. No file reads. Surgical precision:</p>

  <pre><code>Lens.grep(<span class="str">'fetchData'</span>)           <span class="cm">// Find the function</span>
<span class="result">‚Üí "42:async function fetchData"</span>

Lens.setLine(<span class="num">42</span>, <span class="str">'async function fetchData(url) {'</span>)  <span class="cm">// Fix it</span>
<span class="result">‚Üí "‚úì L42"</span>

Lens.save()                        <span class="cm">// Save it</span>
<span class="result">‚Üí "‚úì saved"</span></code></pre>

  <p>Compare this to reading 847 lines, mentally parsing them, generating 847 new lines, and writing them back. <strong>Lens edits at line granularity.</strong></p>

  <h2>How AI Sees</h2>

  <p>When you look at FunctionServer's Studio, you see a desktop with windows, icons, and buttons. When AI looks through Lens, it sees something different:</p>

  <div class="vision-compare">
    <div class="human-view">
      <h4>What You See</h4>
      <div class="fake-desktop">
        <div class="fake-window">
          <div class="fake-titlebar">
            <span class="dots"><span></span><span></span><span></span></span>
            <span class="title">Studio - app.js</span>
          </div>
          <div class="fake-toolbar">
            <span class="btn">+ New</span>
            <span class="btn">Save</span>
            <span class="btn active">Run</span>
          </div>
          <div class="fake-editor">
            <div class="line"><span class="ln">1</span>function hello() {</div>
            <div class="line"><span class="ln">2</span>  return "world";</div>
            <div class="line"><span class="ln">3</span>}</div>
          </div>
        </div>
      </div>
      <p class="caption">Pixels, colors, spatial layout</p>
    </div>
    <div class="claude-view">
      <h4>What AI Sees</h4>
      <pre><code>Lens.look(<span class="str">'body'</span>)
<span class="result"> 0123456789012345678901234
0‚îÇStudio - app.js
1‚îÇ+ New    Save    ‚ñ∂ Run
2‚îÇ
3‚îÇ1:function hello() {
4‚îÇ2:  return "world";
5‚îÇ3:}</span>

Lens.at(<span class="num">1</span>, <span class="num">16</span>)
<span class="result">‚Üí BUTTON:"Run" [click]</span></code></pre>
      <p class="caption">Text grid with coordinates + actions</p>
    </div>
  </div>

  <p>The AI doesn't parse pixels or decode screenshots. It receives a <strong>text grid with coordinates</strong>‚Äîevery element addressable by row and column. Want to click "Run"? <code>Lens.click(1, 16)</code>. Want to know what's there first? <code>Lens.at(1, 16)</code>.</p>

  <p>This isn't OCR or computer vision. It's a <strong>native text representation</strong> designed for how language models actually work.</p>

  <h2>The Problem: Tokens Are Expensive</h2>

  <p>When an AI edits code through traditional tools, the workflow looks like this:</p>

  <ol>
    <li>Read entire file (1000+ tokens)</li>
    <li>Parse and understand structure</li>
    <li>Generate entire new file (1000+ tokens)</li>
    <li>Write to disk</li>
    <li>Repeat for every change</li>
  </ol>

  <p>A simple one-line fix costs thousands of tokens. And the AI is blind‚Äîit can't see if the edit worked without reading the file again.</p>

  <h2>The Solution: Surgical Operations</h2>

  <p>Lens provides a different workflow:</p>

  <div class="comparison">
    <div>
      <h4>Traditional (high tokens)</h4>
      <pre><code><span class="cm">// Read entire file</span>
Read file.js  <span class="result">// 847 lines</span>

<span class="cm">// Rewrite entire file</span>
Write file.js <span class="result">// 847 lines</span>

<span class="cm">// ~3000 tokens for one edit</span></code></pre>
    </div>
    <div>
      <h4>Lens (minimal tokens)</h4>
      <pre><code>Lens.grep(<span class="str">'fetchData'</span>)
<span class="result">// ‚Üí "42:async function fetchData"</span>

Lens.setLine(<span class="num">42</span>, <span class="str">'new code'</span>)
<span class="result">// ‚Üí "‚úì L42"</span>

Lens.save()
<span class="result">// ‚Üí "‚úì saved"</span>

<span class="cm">// ~50 tokens total</span></code></pre>
    </div>
  </div>

  <p><strong>That's 60x fewer tokens for the same edit.</strong></p>

  <h2>The Lens API</h2>

  <h3>Code Navigation</h3>

  <table>
    <tr><th>Command</th><th>Purpose</th><th>Output</th></tr>
    <tr><td><code>Lens.code()</code></td><td>View with line numbers</td><td>Numbered lines</td></tr>
    <tr><td><code>Lens.line(42)</code></td><td>Get specific line</td><td>Line content</td></tr>
    <tr><td><code>Lens.line(42, 5)</code></td><td>Get lines 42-46</td><td>5 lines</td></tr>
    <tr><td><code>Lens.grep('pattern')</code></td><td>Find in code</td><td>Matching lines with numbers</td></tr>
  </table>

  <h3>Surgical Editing</h3>

  <table>
    <tr><th>Command</th><th>Purpose</th><th>Output</th></tr>
    <tr><td><code>Lens.setLine(42, 'code')</code></td><td>Replace one line</td><td><code>‚úì L42</code></td></tr>
    <tr><td><code>Lens.insertLine(42, 'code')</code></td><td>Insert at line</td><td><code>‚úì +L42</code></td></tr>
    <tr><td><code>Lens.deleteLine(42)</code></td><td>Delete line</td><td><code>‚úì -L42</code></td></tr>
    <tr><td><code>Lens.replace('a', 'b')</code></td><td>Find/replace all</td><td><code>‚úì Replaced</code></td></tr>
  </table>

  <h3>DOM Viewport</h3>

  <p>Lens can render any DOM element to a compact text grid:</p>

  <pre><code>Lens.look(<span class="str">'#studio-container'</span>, {width: <span class="num">60</span>, height: <span class="num">20</span>})
<span class="result">// ‚Üí "127 elements
//   0123456789012345678901234567890123456789
//  0‚îÇ+ New              Examples...        ‚ñ∂ Run  üíæ
//  1‚îÇFiles   ‚Üª         API Reference       Git
//  2‚îÇüìÑ app.js          ALGO.createWindow   Commit
//  3‚îÇüìÑ utils.js        Create a new window History
// ..."</span></code></pre>

  <p>The AI can "see" the UI without parsing verbose HTML. Then target elements by position:</p>

  <pre><code>Lens.click(<span class="num">2</span>, <span class="num">3</span>)   <span class="cm">// Click "app.js" at row 2, col 3</span>
<span class="result">// ‚Üí "‚úì clicked"</span></code></pre>

  <h3>Position Inspection</h3>

  <p>Discover what actions are available at any position:</p>

  <pre><code>Lens.at(<span class="num">2</span>, <span class="num">3</span>)   <span class="cm">// What's at row 2, col 3?</span>
<span class="result">// ‚Üí "A:\"app.js\" [click,href:/files/app.js]"</span></code></pre>

  <p>Or list all interactive elements in a container:</p>

  <pre><code>Lens.actions(<span class="str">'#toolbar'</span>)
<span class="result">// ‚Üí "0:button:\"Run\"
//    1:button:\"Save\"
//    2:a:\"Settings\"
//    3:input:\"Search...\""</span>

Lens.do(<span class="str">'#toolbar'</span>, <span class="num">0</span>)  <span class="cm">// Click "Run" button</span>
<span class="result">// ‚Üí "‚úì clicked"</span></code></pre>

  <p><strong>Why this matters:</strong> Instead of guessing what elements exist or parsing HTML, the AI can inspect a location and see exactly what actions are available. This enables exploration of unfamiliar UIs.</p>

  <h3>State & Helpers</h3>

  <table>
    <tr><th>Command</th><th>Purpose</th><th>Output</th></tr>
    <tr><td><code>Lens.state()</code></td><td>Compact system state</td><td><code>w:Studio|Shell e:0 u:william</code></td></tr>
    <tr><td><code>Lens.dash()</code></td><td>Dashboard with icons</td><td><code>[Studio|Shell] üéÆüìùüíª</code></td></tr>
    <tr><td><code>Lens.define(name, fn)</code></td><td>Register helper</td><td><code>‚úì name</code></td></tr>
    <tr><td><code>Lens.call(name, args)</code></td><td>Call helper</td><td>Result</td></tr>
    <tr><td><code>Lens.look(sel, opts)</code></td><td>Render to text grid</td><td>Numbered rows</td></tr>
    <tr><td><code>Lens.at(row, col)</code></td><td>What's at position?</td><td>Element + actions</td></tr>
    <tr><td><code>Lens.click(row, col)</code></td><td>Click at position</td><td><code>‚úì clicked "text"</code></td></tr>
    <tr><td><code>Lens.actions(sel)</code></td><td>List all interactive</td><td>Indexed action list</td></tr>
    <tr><td><code>Lens.do(sel, index)</code></td><td>Execute by index</td><td><code>‚úì clicked</code></td></tr>
  </table>

  <h2>Persistent Helpers</h2>

  <p>Define reusable functions that persist across calls:</p>

  <pre><code>Lens.define(<span class="str">'pos'</span>, () => `P(${px},${py})`)
<span class="result">// ‚Üí "‚úì pos"</span>

Lens.call(<span class="str">'pos'</span>)
<span class="result">// ‚Üí "P(3,5)"</span>

<span class="cm">// Define more complex helpers</span>
Lens.define(<span class="str">'move'</span>, <span class="str">'(dx,dy) => { px+=dx; py+=dy; render(); return Lens.call("pos"); }'</span>)
Lens.call(<span class="str">'move'</span>, <span class="num">1</span>, <span class="num">0</span>)
<span class="result">// ‚Üí "P(4,5)"</span></code></pre>

  <p><strong>Why helpers matter:</strong> Instead of re-sending the same code every call, define it once. Reduces token usage by 80%+ for repeated operations.</p>

  <h2>Batch Operations</h2>

  <p>Chain multiple operations in a single call:</p>

  <pre><code>Lens.batch([
  [<span class="str">'setLine'</span>, <span class="num">42</span>, <span class="str">'const x = await fetch(url);'</span>],
  [<span class="str">'setLine'</span>, <span class="num">43</span>, <span class="str">'const data = await x.json();'</span>],
  [<span class="str">'save'</span>],
  [<span class="str">'run'</span>]
])
<span class="result">// ‚Üí "‚úìsetLine ‚úìsetLine ‚úìsave ‚úìrun"</span></code></pre>

  <p><strong>Four operations. One API call. One response.</strong></p>

  <h2>Compact State</h2>

  <p>Get system state in minimal tokens:</p>

  <pre><code>Lens.state()
<span class="result">// ‚Üí "w:Studio|Shell e:0 u:william"</span>

Lens.dash()
<span class="result">// ‚Üí "[Studio|Shell|(Settings)] üéÆüìùüíªüåê‚öôÔ∏è"</span></code></pre>

  <p>One line tells you: active windows, minimized windows (in parens), error count, current user, and available apps. No JSON parsing needed.</p>

  <h2>Why This Matters</h2>

  <blockquote>
    <p>"The best interface for AI isn't a better API. It's an interface designed from scratch for how AI actually works‚Äîin tokens, not pixels."</p>
  </blockquote>

  <p>Lens inverts the traditional assumption. Instead of adapting AI to human interfaces, we built an interface for AI that humans can also use.</p>

  <h3>For AI:</h3>
  <ul>
    <li>See code with line numbers for addressing</li>
    <li>Edit surgically without reading/rewriting entire files</li>
    <li>See UI state without parsing HTML</li>
    <li>Know immediately if edits worked</li>
    <li>Batch operations to minimize round-trips</li>
  </ul>

  <h3>For Humans:</h3>
  <ul>
    <li>Watch AI navigate code in real-time</li>
    <li>See exactly which lines are being touched</li>
    <li>Faster iteration cycles</li>
    <li>Lower costs (fewer tokens = cheaper)</li>
  </ul>

  <h3>Live Collaboration</h3>

  <p>When AI uses Lens, you see everything happen in real-time. Buttons click. Text appears. Windows open. It's not a transcript of what happened‚Äîit's happening right now, in your browser.</p>

  <p>Ask the AI to "open Studio and fix the bug in app.js" and watch as it:</p>
  <ol>
    <li>Opens the Studio window</li>
    <li>Navigates to app.js</li>
    <li>Searches for the bug with <code>Lens.grep('error')</code></li>
    <li>Edits the specific line with <code>Lens.setLine()</code></li>
    <li>Saves and runs to verify</li>
  </ol>

  <p>You can interrupt, redirect, or take over at any point. It's pair programming where your partner can see your screen.</p>

  <hr>

  <h2>Part of a Larger System</h2>

  <p>Lens doesn't work alone. It's part of FunctionServer's AI development environment:</p>

  <h3>Eye: The Bridge</h3>

  <p>Lens runs through <strong>eye</strong>‚Äîa WebSocket bridge that gives AI direct access to the browser's JavaScript VM. No HTTP overhead. Just raw JavaScript execution in ~25ms.</p>

  <pre><code><span class="cm">// From the terminal, execute JS in your browser</span>
eye <span class="str">'Lens.grep("fetchData")'</span>
<span class="result">‚Üí "42:async function fetchData"</span>

eye <span class="str">'Lens.setLine(42, "fixed")'</span>
<span class="result">‚Üí "‚úì L42"</span></code></pre>

  <h3>AI Eyes: Visual Feedback</h3>

  <p>When AI uses Lens, you see it working. <strong>Purple highlights</strong> show what the AI is inspecting. <strong>Green flashes</strong> indicate edits. Watch the AI's focus move across the screen as it searches, reads, and modifies code.</p>

  <h3>Guardian: Error Awareness</h3>

  <p>Guardian monitors the console. When errors occur, it offers AI assistance. The AI can investigate, use Lens to navigate to the problem, fix it surgically, and verify the fix‚Äîall proactively.</p>

  <h3>GitHub: Ship, Don't Just Generate</h3>

  <p>After editing with Lens, AI can commit and push:</p>

  <pre><code>Lens.save()                              <span class="cm">// Save changes</span>
Studio.github.commit(<span class="str">"Fix fetchData"</span>)    <span class="cm">// Commit</span>
GitHubAuth.push(projectPath)             <span class="cm">// Push to origin</span></code></pre>

  <p>From edit to deployed in three calls.</p>

  <hr>

  <h2>The Complete Workflow</h2>

  <p>Here's what AI-first development looks like:</p>

  <ol>
    <li><strong>Open project</strong> ‚Äî <code>getFileFromDisk("~/repos/myapp/app.js")</code></li>
    <li><strong>Find the code</strong> ‚Äî <code>Lens.grep("handleSubmit")</code></li>
    <li><strong>Understand context</strong> ‚Äî <code>Lens.line(42, 10)</code></li>
    <li><strong>Edit surgically</strong> ‚Äî <code>Lens.setLine(45, "fixed code")</code></li>
    <li><strong>Run and verify</strong> ‚Äî <code>Lens.run()</code></li>
    <li><strong>Save</strong> ‚Äî <code>Lens.save()</code></li>
    <li><strong>Commit and push</strong> ‚Äî <code>Studio.github.commit("message")</code></li>
  </ol>

  <p>Each step is one call. Each call returns immediate confirmation. The debugging loop collapses from minutes to seconds.</p>

  <hr>

  <h2>Design Principles</h2>

  <p>The ideas behind Lens generalize beyond code editing:</p>

  <ul>
    <li><strong>Addressable content</strong> ‚Äî Everything has coordinates (line numbers, row/col positions)</li>
    <li><strong>Compact output</strong> ‚Äî Status in one line, not JSON blobs</li>
    <li><strong>Surgical operations</strong> ‚Äî Edit the smallest possible unit</li>
    <li><strong>Batch support</strong> ‚Äî Multiple operations per call</li>
    <li><strong>Verification built-in</strong> ‚Äî Every operation confirms success</li>
    <li><strong>Visual feedback</strong> ‚Äî Humans see what AI is doing</li>
  </ul>

  <p>These principles apply to any interface where AI and humans collaborate in real-time.</p>

  <hr>

  <div class="cta-section">
    <p><strong>Try it:</strong> <a href="/app">functionserver.com/app</a></p>
    <p><strong>The Happy Path:</strong> <a href="/thehappypath.html">AI-first development guide</a></p>
    <p><strong>The Door:</strong> <a href="/door.html">Architecture philosophy</a></p>
    <p><strong>Code:</strong> <a href="https://github.com/williamsharkey/functionserver">github.com/williamsharkey/functionserver</a></p>
  </div>

</article>

<footer>
  <a href="/">FunctionServer</a> ¬∑ Open source under MIT
</footer>

</body>
</html>
